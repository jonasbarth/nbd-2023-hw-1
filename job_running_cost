#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr 18 15:34:00 2023

@author: giacomo
"""
import numpy as np

import matplotlib.pyplot as plt
import fat_tree
from tqdm import tqdm

def simulate_response_time(n_servers):
    
    exec_times = np.random.exponential(expected_job_time_s / n_servers, n_servers)
    
    exec_times += fixed_job_time_s

    main_server = np.random.randint(0, n_servers)
    
    n_closest_servers = tree.n_closest(main_server, n_servers)
    
    for i, server in enumerate(n_closest_servers):
        
        avg_throughput = tree.avg_throughput(main_server, server, n_servers + 1)

        outbound_data_size = input_file_size_gb / n_servers * overhead
        
        outbound_time = outbound_data_size / avg_throughput

        return_size = np.random.uniform(0, 2 * output_file_size_gb / n_servers) * overhead
        
        return_time = return_size / avg_throughput

        exec_times[i] += outbound_time + return_time

    return np.max(exec_times)

def simulate_job_running_cost(n_servers, n_simulations):
    """
    According to the formula, in order to extract the expected value of the 
    response time and of theta, the function does a simulation in which it 
    uses the simulate_response_time function to calculate the response time and 
    it calculate the value of theta (sum of the times that all N servers are
    used to run their respective tasks). After that it extracts the mean and it
    returns the job running cost. 
    
    """
    sum_response_time = 0 
    sum_task_time = 0
    
    for _ in range(n_simulations): 
    
        sum_response_time += simulate_response_time(n_servers)
        
        task_time = 0
        
        for _ in range(n_servers):
            
            exec_times = np.random.exponential(expected_job_time_s / n_servers, n_servers)
            
            task_time += (fixed_job_time_s + exec_times)
            
        sum_task_time += task_time/n_servers
        
    job_running_cost = sum_response_time/n_simulations + (xi * (sum_task_time/n_simulations))
    
    return job_running_cost
        

if __name__ == "__main__":

    N = 10000
    n = 64
    tau_s = 0.000005
    capacity_gbit = 10
    expected_job_time_s = 8 * 3600
    fixed_job_time_s = 30
    input_file_size_gb = 4000
    output_file_size_gb = 4000
    xi = 0.1
    overhead = 1 + 48 / 1500

    baseline = (expected_job_time_s + fixed_job_time_s) + (xi * expected_job_time_s)

    tree = fat_tree.FatTree(n, tau_s, capacity_gbit)
    job_costs = []

    servers = range(1, N + 1, 10)

    for n_servers in tqdm(servers):
        
        job_cost = simulate_job_running_cost(n_servers, 10)
        
        job_costs.append(job_cost)

    plt.plot(servers, np.array(job_costs) / baseline)
    plt.xlabel("Number of Servers")
    plt.ylabel("S")
    plt.title("Normalised Job Running Cost")
    plt.show()
